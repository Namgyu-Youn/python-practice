#lecture/coursera #DL/CNN #ML/ensemble #ML
## Unsupervised Learning
### Overview
- 목표: 데이터의 숨겨진 구조 학습
- 예시: Clustering, Dimension reduction, Density estimation
- 장점: 레이블링 비용 절감 (인력 소요 감소)

## Clustering Methods
### Basic Concepts
- 주관적 기준으로 하위 그룹 생성
- 유형: Hierarchical(계층적 분해, bottom-up), Partitional(다양한 분할 평가, top-down)

### Hierarchical Clustering
- level 0: 단일 클러스터
- level 1~L: 클러스터 간 거리 추정 및 병합
- 문제점: Closest pair(다른 클러스터와 단일 연결), Farthest pair(원거리 클러스터 연결 어려움)

### K-means Algorithm
1. K개 무작위 점을 클러스터 중심으로 선택
2. 할당된 점들의 평균으로 클러스터 중심 변경
3. 평균 중심이 이동하지 않을 때까지 반복
	![[../../../9. AttachedFiles/Pasted image 20250224044317.png]]

### Cluster 수 선정
- 비용 함수에 의미있는 하위 그룹 수 결정
- 방법: Elbow method(SSE 사용, 변곡점 선택), Silhouette method(밀도와 분리도 비교)

## Dimension Reduction
### Manifold Learning
- 대부분의 데이터는 부분 공간에 존재
- 기본 부분 공간 탐색에 중점
- MDS: 유사도 기반 왜곡 최소화

### PCA (Principal Component Analysis)
- 목표: 축소된 차원에서 원본 정보량(분산) 보존
- 과정: 데이터 중심화/정규화 → 공분산 행렬 추정 → EVD 수행/고유값 정렬 → k차원 공간 선택
	![[../../../9. AttachedFiles/Pasted image 20250224044344.png]]

## Classification Methods
### Nearest Neighbors
- 인접 이웃과 비교하여 분류
- 특징: 학습 불필요, 시간 복잡도 O(N), k값 영향(작으면 과적합, 크면 균등 영역)
- 문제점: 픽셀 거리 행렬의 정보성 부족, 높은 계산 비용, 차원의 저주
	![[../../../9. AttachedFiles/Pasted image 20250224044412.png]]

### Linear Classifier
- Parametric approach: 매핑 함수 설정
- 분류를 위한 가중치 계산
- kNN과 비교: 거리 비교(유사점), K개 클래스만 비교 vs N개 학습 예제 비교(차이점)
	![[../../../9. AttachedFiles/Pasted image 20250224044446.png]]

### Softmax Classifier
- Sigmoid 함수 확장
- 다중 분류를 위한 확률 출력
- Cross Entropy(loss) = −∑(yi)log(pi) (yi: 인코딩 값, pi: 클래스 확률)

### Neural Network Issues
- Linear Classifier 한계: 선형 결정 경계만 가능
- 해결책: 특성 추출로 선형 분리 가능성 향상
- 이미지 특성 예: 색상 히스토그램
	![[../../../9. AttachedFiles/Pasted image 20250224044516.png]]