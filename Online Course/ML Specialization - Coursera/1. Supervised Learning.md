#lecture/coursera #ML
## Supervised Learning
- Input(X)로 Output(Y)를 예측
- 레이블이 있는 데이터로 학습
- **Types**: Prediction(근사 선형 관계 찾기), Classification(카테고리 예측)

## Unsupervised Learning
- 레이블이 없는 데이터에서 군집(cluster) 발견
- **Dimensionality Reduction**: 구조를 쉽게 파악
---
## Linear Regression
### Single Linear Regression
- 학습 세트로 알고리즘 학습 → 함수(모델) 도출
- 예측값(y-hat)은 추정된 y
- 선형 특성으로 인해 단일 변수만 사용 가능

### Cost Function (J)
- 모델의 예측값과 실제값 간의 차이 계산
- 비용 함수가 낮을수록 더 나은 모델

### Gradient Descent Algorithm
![[../../../9. AttachedFiles/Pasted image 20250224042426.png]]
- 수렴할 때까지 min(w, b) 반복
- 목표 지점(m)을 향해 기울기를 낮추며 접근
- gradient 감소에 따라 하강 거리도 감소
- **Learning Rate**: w를 향해 접근하는 가중치, 크기에 따라 `Overfitting/Underfitting` 발생 가능
- `StandardScaler`로 계산을 위한 모든 특성 스케일링

### Multi-Linear Regression
- x는 행 벡터(특성들의 군집)
- w, x bar의 곱은 내적 (Python: `np.dot(w,x)` 사용)
---
## Gradient Descent Algorithm
### Feature Scaling
- 파라미터 범위 차이가 크면 가중치 차이도 큼
- 모든 특성을 적절히 적용할 수 없는 문제 해결
- 목표: J(비용 함수) 최소화
- 자동 수렴 테스트: 반복 후 J가 ε(목표점)보다 낮은지 확인

### Normalization Types
- Mean: 모든 데이터를 `CoVar [-1,1]` 사이에 배치
- Z-score: SVD 사용

### Learning Rate (α) 선택
- Overfitting: 너무 높으면 비용이 일관되게 감소하지 않음
- Underfitting: 너무 작으면 반복이 많이 발생

### Polynomial Regression
- 선형 모델이 항상 적합하지는 않음
- 제곱, 루트 값 등 사용
- 결정 방법: Algorithm (Lesson II)
---
## Classification with Logistic Regression
### Classification
- 이진 분류 사용
- Decision Boundary: 분류 경계선

### Logistic Regression
![[../../../9. AttachedFiles/Pasted image 20250224042451.png]]
- 회귀를 사용하여 경계 추정
- Input feature의 가중치 합 측정 후 오차 측정
- 선형 모델의 분류 문제 보완

### Decision Boundary
- sigmoid 함수에서 x>0일 때 p(Y=1)>p(Y=0)로 분류 가능
- 입력(x)이 'wx+b' 또는 다항식이면 경계가 x로 재정의 가능
- 더 많은 다항식 입력은 더 많은 변수 특성을 의미

## Overfitting and Solutions
- 모델에 지나치게 잘 맞는 현상
- Solutions : 더 많은 데이터 수집, 특성 포함/제외, 정규화(Regularization)

### Regularization
- 과도하게 큰 파라미터 크기 감소 → 균형(가중치)
- 유형: L1 (Lasso), L2 (Ridge), Elastic Net
	![[../../../9. AttachedFiles/Pasted image 20250224042556.png]]